{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set-up"
      ],
      "metadata": {
        "id": "8IZ_8E8rTbbJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn9QqALbxHw0",
        "outputId": "4545cb25-7295-4842-8b60-3bd5cb0c0917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client sentence-transformers transformers torch accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXaMYBtwxNoB",
        "outputId": "9662302c-aa34-47f7-ede7-646d68c0a249"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.75.1)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
            "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.11.9)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Downloading qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, qdrant-client\n",
            "Successfully installed portalocker-3.2.0 qdrant-client-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Evaluation"
      ],
      "metadata": {
        "id": "dIWL6CozwbXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ########### Testing cosine similarity ##########\n",
        "\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# # Create vectors\n",
        "# q1 = embedder.encode(\"I want biryani\")\n",
        "# r1 = embedder.encode(\"chicken biryani recipe\")\n",
        "# r2 = embedder.encode(\"chocolate cake recipe\")\n",
        "\n",
        "# # Calculate similarity\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import numpy as np\n",
        "\n",
        "# sim1 = cosine_similarity([q1], [r1])[0][0]\n",
        "# sim2 = cosine_similarity([q1], [r2])[0][0]\n",
        "\n",
        "# print(f\"Biryani query vs Biryani recipe: {sim1:.3f}\")\n",
        "# print(f\"Biryani query vs Cake recipe:    {sim2:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz7xE3IcT5f5",
        "outputId": "aeb79393-3185-4579-97ac-2d8a456c9592"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Biryani query vs Biryani recipe: 0.564\n",
            "Biryani query vs Cake recipe:    0.166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Methodology\n",
        "\n",
        "\n",
        "#### **1. Ground Truth Creation**\n",
        "\n",
        "* A set of **20 test queries** was generated directly from the recipe database (ground_truth.csv).\n",
        "* Queries covered:\n",
        "\n",
        "  * **Exact recipe names** (e.g., *\"Hyderabadi Chicken Biryani\"*)\n",
        "  * **Ingredient-based queries** (e.g., *\"curry with lentils and spinach\"*)\n",
        "* Since recipe names may vary (e.g., *\"carrot cake\"* vs *\"carrot cake II\"*), **fuzzy string matching with a threshold of 0.65** was used to recognize near matches as correct.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Metrics Chosen**\n",
        "\n",
        "Multiple retrieval evaluation metrics were applied to account for both accuracy and user experience:\n",
        "\n",
        "* **Hit Rate@K**\n",
        "\n",
        "  * Measures whether the correct recipe appears in the top-K results.\n",
        "  * Example: *Hit Rate@1* requires that the correct recipe be ranked first.\n",
        "\n",
        "* **MRR (Mean Reciprocal Rank)**\n",
        "\n",
        "  * Rewards higher-ranking correct results.\n",
        "  * Rank 1 → score = 1.0\n",
        "  * Rank 5 → score = 0.2\n",
        "\n",
        "* **Cosine Similarity**\n",
        "\n",
        "  * Evaluates semantic similarity between the retrieved recipe embedding and the ground truth embedding.\n",
        "  * Especially useful for ingredient-based queries where exact text matches are less reliable.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Metrics**\n",
        "\n",
        "* **Hit Rate@1** reflects **immediate user satisfaction**, indicating whether the desired recipe is found instantly.\n",
        "* **Hit Rate@3** reflects **realistic browsing behavior**, where users typically check the first few results.\n",
        "* **MRR** provides a **single aggregated score** that accounts for ranking position.\n",
        "* **Cosine Similarity** ensures that retrieval captures **semantic closeness**, not only textual overlap.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NyGVyNAkEphL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n",
        "# Connect to Qdrant\n",
        "QDRANT_URL = \"xxxxxxx\"\n",
        "QDRANT_API_KEY = \"xxxxxx\"\n",
        "qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
        "\n",
        "# Load embedding model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RAG EVALUATION - Testing Recipe Retrieval Quality\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "\n",
        "# Fuzzy String Matching\n",
        "\n",
        "\n",
        "def fuzzy_match(str1, str2, threshold=0.7):\n",
        "    \"\"\"\n",
        "    String Similarity – Evaluate if two strings are close enough\n",
        "\n",
        "    Examples:\n",
        "    - \"biryani\" vs \"hyderabadi chicken biryani\" → 0.55 similarity\n",
        "    - \"best lemonade\" vs \"best lemonade\" → 1.0 similarity\n",
        "    - \"carrot cake ii\" vs \"liz s famous carrot cake\" → 0.45 similarity\n",
        "    \"\"\"\n",
        "    str1 = str1.lower().strip()\n",
        "    str2 = str2.lower().strip()\n",
        "\n",
        "    # Exact match\n",
        "    if str1 == str2:\n",
        "        return True\n",
        "\n",
        "    # One contains the other\n",
        "    if str1 in str2 or str2 in str1:\n",
        "        return True\n",
        "\n",
        "    # Fuzzy similarity\n",
        "    similarity = SequenceMatcher(None, str1, str2).ratio()\n",
        "    return similarity >= threshold\n",
        "\n",
        "\n",
        "\n",
        "# 1: Load Ground Truth Test Data...............................................\n",
        "\n",
        "\n",
        "def load_test_data():\n",
        "    \"\"\"Load ground truth CSV\"\"\"\n",
        "    df = pd.read_csv('ground_truth.csv')  # grouth truth file\n",
        "    print(f\"\\n Loaded {len(df)} test questions\")\n",
        "    print(f\"\\nQuery type breakdown:\")\n",
        "    print(df['query_type'].value_counts())\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# 2: Retrieve Top-K Recipes for Each Question..................................\n",
        "\n",
        "\n",
        "def retrieve_top_k(question, k=3):\n",
        "    \"\"\"\n",
        "    Retrieve top-k most similar recipes for a question\n",
        "\n",
        "    Returns:\n",
        "        list: [(recipe_id, recipe_name, score), ...]\n",
        "    \"\"\"\n",
        "    # Convert question to vector\n",
        "    query_vector = embedder.encode(question).tolist()\n",
        "\n",
        "    # Search Qdrant\n",
        "    results = qdrant_client.query_points(\n",
        "        collection_name=\"recipes\",\n",
        "        query=query_vector,\n",
        "        limit=k\n",
        "    )\n",
        "\n",
        "    # Extract results\n",
        "    retrieved = []\n",
        "    for point in results.points:\n",
        "        recipe_id = point.payload.get('id')\n",
        "        recipe_name = point.payload.get('name', 'Unknown')\n",
        "        score = point.score  # Cosine similarity score\n",
        "        retrieved.append((recipe_id, recipe_name, score))\n",
        "\n",
        "    return retrieved\n",
        "\n",
        "\n",
        "\n",
        "# 3: Calculate Evaluation Metrics (WITH FUZZY MATCHING)........................\n",
        "\n",
        "\n",
        "def calculate_hit_rate_at_k(results, k=1):\n",
        "    \"\"\"\n",
        "    Hit Rate@k: % of queries where correct recipe is in top-k results\n",
        "    Uses FUZZY NAME MATCHING instead of exact ID matching\n",
        "    \"\"\"\n",
        "    hits = 0\n",
        "\n",
        "    for result in results:\n",
        "        if result['hit_at_k'][f'hit_at_{k}']:\n",
        "            hits += 1\n",
        "\n",
        "    return hits / len(results) if results else 0\n",
        "\n",
        "\n",
        "def calculate_mrr(results):\n",
        "    \"\"\"\n",
        "    MRR (Mean Reciprocal Rank): Average of 1/rank for correct recipes\n",
        "    \"\"\"\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for result in results:\n",
        "        rank = result.get('correct_recipe_rank', 0)\n",
        "        if rank > 0:\n",
        "            reciprocal_ranks.append(1.0 / rank)\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)\n",
        "\n",
        "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "\n",
        "\n",
        "def calculate_average_score(results):\n",
        "    \"\"\"\n",
        "    Average cosine similarity score for top-1 results\n",
        "    \"\"\"\n",
        "    scores = [r['top1_score'] for r in results]\n",
        "    return sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "\n",
        "\n",
        "# 4: Run RAG Evaluation (WITH FUZZY MATCHING)...............................\n",
        "\n",
        "\n",
        "def run_rag_evaluation(test_df, top_k=3, fuzzy_threshold=0.65):\n",
        "    \"\"\"\n",
        "    Main evaluation function\n",
        "    Uses fuzzy name matching to determine if retrieval is correct\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\" Running RAG Evaluation on {len(test_df)} questions...\")\n",
        "    print(f\"   Using fuzzy matching threshold: {fuzzy_threshold}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        question = row['question']\n",
        "        expected_name = row['expected_name']\n",
        "        query_type = row['query_type']\n",
        "\n",
        "        # Retrieve top-k recipes\n",
        "        retrieved = retrieve_top_k(question, k=top_k)\n",
        "        retrieved_names = [r[1] for r in retrieved]\n",
        "        scores = [r[2] for r in retrieved]\n",
        "\n",
        "        # Find if expected recipe is in top-k (using fuzzy matching)\n",
        "        correct_rank = 0\n",
        "        for rank, (_, name, _) in enumerate(retrieved, 1):\n",
        "            if fuzzy_match(expected_name, name, threshold=fuzzy_threshold):\n",
        "                correct_rank = rank\n",
        "                break\n",
        "\n",
        "        # Check hits at different k values\n",
        "        is_correct_at_1 = (correct_rank == 1)\n",
        "        is_correct_at_3 = (correct_rank > 0 and correct_rank <= 3)\n",
        "        is_correct_at_5 = (correct_rank > 0 and correct_rank <= 5)\n",
        "\n",
        "        # Store result\n",
        "        result = {\n",
        "            'question': question,\n",
        "            'query_type': query_type,\n",
        "            'expected_name': expected_name,\n",
        "            'retrieved_names': retrieved_names,\n",
        "            'top1_score': scores[0] if scores else 0,\n",
        "            'all_scores': scores,\n",
        "            'correct_recipe_rank': correct_rank,\n",
        "            'hit_at_k': {\n",
        "                'hit_at_1': is_correct_at_1,\n",
        "                'hit_at_3': is_correct_at_3,\n",
        "                'hit_at_5': is_correct_at_5\n",
        "            }\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Print progress\n",
        "        if is_correct_at_1:\n",
        "            status = \"Yes\"\n",
        "        elif is_correct_at_3:\n",
        "            status = f\" (rank {correct_rank})\"\n",
        "        else:\n",
        "            status = \"No\"\n",
        "\n",
        "        print(f\"{status} Q{idx+1:2d}: {question[:45]:45s} | Got: {retrieved_names[0][:35]}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# 5: Results....................................................................\n",
        "\n",
        "\n",
        "def display_results(results):\n",
        "    \"\"\"\n",
        "    Display comprehensive evaluation results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RAG EVALUATION RESULTS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Overall Metrics\n",
        "    hit_rate_1 = calculate_hit_rate_at_k(results, k=1) * 100\n",
        "    hit_rate_3 = calculate_hit_rate_at_k(results, k=3) * 100\n",
        "    mrr = calculate_mrr(results)\n",
        "    avg_score = calculate_average_score(results)\n",
        "\n",
        "    total = len(results)\n",
        "    correct_at_1 = sum(1 for r in results if r['hit_at_k']['hit_at_1'])\n",
        "    correct_at_3 = sum(1 for r in results if r['hit_at_k']['hit_at_3'])\n",
        "\n",
        "    print(\" OVERALL METRICS:\")\n",
        "    print(f\"   • Hit Rate@1:  {hit_rate_1:.1f}% ({correct_at_1}/{total} questions)\")\n",
        "    print(f\"   • Hit Rate@3:  {hit_rate_3:.1f}% ({correct_at_3}/{total} questions)\")\n",
        "    print(f\"   • MRR (Mean Reciprocal Rank): {mrr:.3f}\")\n",
        "    print(f\"   • Average Top-1 Similarity:   {avg_score:.3f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(\"\\n INTERPRETATION:\")\n",
        "    if hit_rate_1 >= 80:\n",
        "        print(\" EXCELLENT - Retrieval works very well!\")\n",
        "    elif hit_rate_1 >= 60:\n",
        "        print(\"  GOOD - Retrieval is performing well\")\n",
        "    else:\n",
        "        print(\" NEEDS IMPROVEMENT \")\n",
        "\n",
        "    # Breakdown by Query Type\n",
        "    print(f\"\\nBREAKDOWN BY QUERY TYPE:\")\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    for query_type in df['query_type'].unique():\n",
        "        subset = df[df['query_type'] == query_type]\n",
        "        hit_1 = sum(1 for _, r in subset.iterrows() if r['hit_at_k']['hit_at_1']) / len(subset) * 100\n",
        "        hit_3 = sum(1 for _, r in subset.iterrows() if r['hit_at_k']['hit_at_3']) / len(subset) * 100\n",
        "        count = len(subset)\n",
        "        print(f\"   • {query_type:20s}: Hit@1={hit_1:5.1f}%, Hit@3={hit_3:5.1f}% (n={count})\")\n",
        "\n",
        "    # Show Complete Failures (not even in top-3)\n",
        "    failures = [r for r in results if not r['hit_at_k']['hit_at_3']]\n",
        "    if failures:\n",
        "        print(f\"\\n COMPLETE FAILURES ({len(failures)} questions - not in top-3):\")\n",
        "        for f in failures[:5]:  # Show first 5\n",
        "            print(f\"\\n   Question: {f['question']}\")\n",
        "            print(f\"   Expected: {f['expected_name']}\")\n",
        "            print(f\"   Got:      {f['retrieved_names'][0]}\")\n",
        "            if len(f['retrieved_names']) > 1:\n",
        "                print(f\"             {f['retrieved_names'][1]}\")\n",
        "\n",
        "    # Near Misses (correct in top-3 but not top-1)\n",
        "    near_misses = [r for r in results if r['hit_at_k']['hit_at_3'] and not r['hit_at_k']['hit_at_1']]\n",
        "    if near_misses:\n",
        "        print(f\"\\n NEAR MISSES ({len(near_misses)} questions - correct in top-3 but not top-1):\")\n",
        "        for nm in near_misses[:5]:  # Show first 5\n",
        "            rank = nm['correct_recipe_rank']\n",
        "            print(f\"\\n   Question: {nm['question']}\")\n",
        "            print(f\"   Expected: {nm['expected_name']} (found at rank {rank})\")\n",
        "            print(f\"   Top-1:    {nm['retrieved_names'][0]}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# 6: Save Results\n",
        "\n",
        "\n",
        "def save_results(results_df):\n",
        "    \"\"\"Save evaluation results to CSV\"\"\"\n",
        "    output_file = 'rag_evaluation_results.csv'\n",
        "\n",
        "    # Flatten for CSV\n",
        "    save_df = pd.DataFrame({\n",
        "        'question': results_df['question'],\n",
        "        'query_type': results_df['query_type'],\n",
        "        'expected_name': results_df['expected_name'],\n",
        "        'retrieved_top1': results_df['retrieved_names'].apply(lambda x: x[0] if x else ''),\n",
        "        'hit_at_1': results_df['hit_at_k'].apply(lambda x: x['hit_at_1']),\n",
        "        'hit_at_3': results_df['hit_at_k'].apply(lambda x: x['hit_at_3']),\n",
        "        'correct_rank': results_df['correct_recipe_rank'],\n",
        "        'top1_score': results_df['top1_score']\n",
        "    })\n",
        "\n",
        "    save_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\n Results saved to: {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "# MAIN EXECUTION\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load test data\n",
        "    test_df = load_test_data()\n",
        "\n",
        "    # Run evaluation with fuzzy matching\n",
        "    results = run_rag_evaluation(test_df, top_k=3, fuzzy_threshold=0.65)\n",
        "\n",
        "    # Display and save\n",
        "    results_df = display_results(results)\n",
        "    save_results(results_df)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RAG EVALUATION COMPLETE!\")\n",
        "    print(f\"{'='*70}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5WDPzPd1jSh",
        "outputId": "8209a4fe-d7ae-44f3-a0d2-e78e481512f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "RAG EVALUATION - Testing Recipe Retrieval Quality\n",
            "======================================================================\n",
            "\n",
            " Loaded 20 test questions\n",
            "\n",
            "Query type breakdown:\n",
            "query_type\n",
            "exact_name          17\n",
            "ingredient_based     3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "======================================================================\n",
            " Running RAG Evaluation on 20 questions...\n",
            "   Using fuzzy matching threshold: 0.65\n",
            "======================================================================\n",
            "\n",
            "Yes Q 1: Show me low fat berry blue frozen dessert     | Got: low fat berry blue frozen dessert\n",
            "Yes Q 2: I want biryani                                | Got: hyderabadi chicken biryani\n",
            "Yes Q 3: Show me best lemonade                         | Got: the best  lemonade ever\n",
            "Yes Q 4: carina s tofu vegetable kebabs recipe         | Got: carina s tofu vegetable kebabs\n",
            "Yes Q 5: best blackbottom pie recipe                   | Got: best blackbottom pie\n",
            "Yes Q 6: How to make buttermilk pie with gingersnap cr | Got: buttermilk pie with gingersnap crum\n",
            "Yes Q 7: I want a jad   cucumber pickle                | Got: a jad   cucumber pickle\n",
            "Yes Q 8: Show me boston cream pie                      | Got: boston cream  creme  pie\n",
            "Yes Q 9: I want chicken breasts lombardi               | Got: chicken breasts lombardi\n",
            "Yes Q10: Show me biscotti di prato                     | Got: biscotti di prato\n",
            "Yes Q11: cafe cappuccino recipe                        | Got: cafe cappuccino\n",
            "Yes Q12: How to make jimmy g s carrot cake             | Got: jimmy g s carrot cake\n",
            "Yes Q13: How to make betty crocker s southwestern guac | Got: betty crocker s southwestern guacam\n",
            "Yes Q14: How to make low fat burgundy beef   vegetable | Got: low fat burgundy beef   vegetable s\n",
            "Yes Q15: How to make lou s fabulous bruschetta         | Got: lou s fabulous bruschetta\n",
            " (rank 2) Q16: I want black bean  corn  and tomato salad     | Got: corn  tomato and black bean salad\n",
            "Yes Q17: How to make cabbage and sausage soup          | Got: cabbage sausage soup\n",
            "Yes Q18: Show me black coffee barbecue sauce           | Got: black coffee barbecue sauce\n",
            "Yes Q19: How to make bourbon pecan pound cake          | Got: bourbon pecan pound cake\n",
            "Yes Q20: How to make buckwheat bread                   | Got: buckwheat bread\n",
            "\n",
            "======================================================================\n",
            "RAG EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            " OVERALL METRICS:\n",
            "   • Hit Rate@1:  95.0% (19/20 questions)\n",
            "   • Hit Rate@3:  100.0% (20/20 questions)\n",
            "   • MRR (Mean Reciprocal Rank): 0.975\n",
            "   • Average Top-1 Similarity:   0.748\n",
            "\n",
            " INTERPRETATION:\n",
            " EXCELLENT - Retrieval works very well!\n",
            "\n",
            "BREAKDOWN BY QUERY TYPE:\n",
            "   • exact_name          : Hit@1= 94.1%, Hit@3=100.0% (n=17)\n",
            "   • ingredient_based    : Hit@1=100.0%, Hit@3=100.0% (n=3)\n",
            "\n",
            " NEAR MISSES (1 questions - correct in top-3 but not top-1):\n",
            "\n",
            "   Question: I want black bean  corn  and tomato salad\n",
            "   Expected: black bean  corn  and tomato salad (found at rank 2)\n",
            "   Top-1:    corn  tomato and black bean salad\n",
            "\n",
            " Results saved to: rag_evaluation_results.csv\n",
            "\n",
            "======================================================================\n",
            "RAG EVALUATION COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with different embedding model------------paraphrase-MiniLM-L6-v2 (optimized for paraphrases)---------uses 384 dimensions-----compatible with qdrant\n",
        "\n",
        "\n",
        "# Load embedding model\n",
        "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RAG EVALUATION - Testing Recipe Retrieval Quality\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "\n",
        "# Fuzzy String Matching\n",
        "\n",
        "\n",
        "def fuzzy_match(str1, str2, threshold=0.7):\n",
        "    \"\"\"\n",
        "    String Similarity – Evaluate if two strings are close enough\n",
        "\n",
        "    Examples:\n",
        "    - \"biryani\" vs \"hyderabadi chicken biryani\" → 0.55 similarity\n",
        "    - \"best lemonade\" vs \"best lemonade\" → 1.0 similarity\n",
        "    - \"carrot cake ii\" vs \"liz s famous carrot cake\" → 0.45 similarity\n",
        "    \"\"\"\n",
        "    str1 = str1.lower().strip()\n",
        "    str2 = str2.lower().strip()\n",
        "\n",
        "    # Exact match\n",
        "    if str1 == str2:\n",
        "        return True\n",
        "\n",
        "    # One contains the other\n",
        "    if str1 in str2 or str2 in str1:\n",
        "        return True\n",
        "\n",
        "    # Fuzzy similarity\n",
        "    similarity = SequenceMatcher(None, str1, str2).ratio()\n",
        "    return similarity >= threshold\n",
        "\n",
        "\n",
        "\n",
        "# 1: Load Ground Truth Test Data...............................................\n",
        "\n",
        "\n",
        "def load_test_data():\n",
        "    \"\"\"Load ground truth CSV\"\"\"\n",
        "    df = pd.read_csv('ground_truth.csv')  # grouth truth file\n",
        "    print(f\"\\n Loaded {len(df)} test questions\")\n",
        "    print(f\"\\nQuery type breakdown:\")\n",
        "    print(df['query_type'].value_counts())\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# 2: Retrieve Top-K Recipes for Each Question..................................\n",
        "\n",
        "\n",
        "def retrieve_top_k(question, k=3):\n",
        "    \"\"\"\n",
        "    Retrieve top-k most similar recipes for a question\n",
        "\n",
        "    Returns:\n",
        "        list: [(recipe_id, recipe_name, score), ...]\n",
        "    \"\"\"\n",
        "    # Convert question to vector\n",
        "    query_vector = embedder.encode(question).tolist()\n",
        "\n",
        "    # Search Qdrant\n",
        "    results = qdrant_client.query_points(\n",
        "        collection_name=\"recipes\",\n",
        "        query=query_vector,\n",
        "        limit=k\n",
        "    )\n",
        "\n",
        "    # Extract results\n",
        "    retrieved = []\n",
        "    for point in results.points:\n",
        "        recipe_id = point.payload.get('id')\n",
        "        recipe_name = point.payload.get('name', 'Unknown')\n",
        "        score = point.score  # Cosine similarity score\n",
        "        retrieved.append((recipe_id, recipe_name, score))\n",
        "\n",
        "    return retrieved\n",
        "\n",
        "\n",
        "\n",
        "# 3: Calculate Evaluation Metrics (WITH FUZZY MATCHING)........................\n",
        "\n",
        "\n",
        "def calculate_hit_rate_at_k(results, k=1):\n",
        "    \"\"\"\n",
        "    Hit Rate@k: % of queries where correct recipe is in top-k results\n",
        "    Uses FUZZY NAME MATCHING instead of exact ID matching\n",
        "    \"\"\"\n",
        "    hits = 0\n",
        "\n",
        "    for result in results:\n",
        "        if result['hit_at_k'][f'hit_at_{k}']:\n",
        "            hits += 1\n",
        "\n",
        "    return hits / len(results) if results else 0\n",
        "\n",
        "\n",
        "def calculate_mrr(results):\n",
        "    \"\"\"\n",
        "    MRR (Mean Reciprocal Rank): Average of 1/rank for correct recipes\n",
        "    \"\"\"\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for result in results:\n",
        "        rank = result.get('correct_recipe_rank', 0)\n",
        "        if rank > 0:\n",
        "            reciprocal_ranks.append(1.0 / rank)\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)\n",
        "\n",
        "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "\n",
        "\n",
        "def calculate_average_score(results):\n",
        "    \"\"\"\n",
        "    Average cosine similarity score for top-1 results\n",
        "    \"\"\"\n",
        "    scores = [r['top1_score'] for r in results]\n",
        "    return sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "\n",
        "\n",
        "# 4: Run RAG Evaluation (WITH FUZZY MATCHING)...............................\n",
        "\n",
        "\n",
        "def run_rag_evaluation(test_df, top_k=3, fuzzy_threshold=0.65):\n",
        "    \"\"\"\n",
        "    Main evaluation function\n",
        "    Uses fuzzy name matching to determine if retrieval is correct\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\" Running RAG Evaluation on {len(test_df)} questions...\")\n",
        "    print(f\"   Using fuzzy matching threshold: {fuzzy_threshold}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        question = row['question']\n",
        "        expected_name = row['expected_name']\n",
        "        query_type = row['query_type']\n",
        "\n",
        "        # Retrieve top-k recipes\n",
        "        retrieved = retrieve_top_k(question, k=top_k)\n",
        "        retrieved_names = [r[1] for r in retrieved]\n",
        "        scores = [r[2] for r in retrieved]\n",
        "\n",
        "        # Find if expected recipe is in top-k (using fuzzy matching)\n",
        "        correct_rank = 0\n",
        "        for rank, (_, name, _) in enumerate(retrieved, 1):\n",
        "            if fuzzy_match(expected_name, name, threshold=fuzzy_threshold):\n",
        "                correct_rank = rank\n",
        "                break\n",
        "\n",
        "        # Check hits at different k values\n",
        "        is_correct_at_1 = (correct_rank == 1)\n",
        "        is_correct_at_3 = (correct_rank > 0 and correct_rank <= 3)\n",
        "        is_correct_at_5 = (correct_rank > 0 and correct_rank <= 5)\n",
        "\n",
        "        # Store result\n",
        "        result = {\n",
        "            'question': question,\n",
        "            'query_type': query_type,\n",
        "            'expected_name': expected_name,\n",
        "            'retrieved_names': retrieved_names,\n",
        "            'top1_score': scores[0] if scores else 0,\n",
        "            'all_scores': scores,\n",
        "            'correct_recipe_rank': correct_rank,\n",
        "            'hit_at_k': {\n",
        "                'hit_at_1': is_correct_at_1,\n",
        "                'hit_at_3': is_correct_at_3,\n",
        "                'hit_at_5': is_correct_at_5\n",
        "            }\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Print progress\n",
        "        if is_correct_at_1:\n",
        "            status = \"Yes\"\n",
        "        elif is_correct_at_3:\n",
        "            status = f\" (rank {correct_rank})\"\n",
        "        else:\n",
        "            status = \"No\"\n",
        "\n",
        "        print(f\"{status} Q{idx+1:2d}: {question[:45]:45s} | Got: {retrieved_names[0][:35]}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# 5: Results....................................................................\n",
        "\n",
        "\n",
        "def display_results(results):\n",
        "    \"\"\"\n",
        "    Display comprehensive evaluation results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RAG EVALUATION RESULTS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Overall Metrics\n",
        "    hit_rate_1 = calculate_hit_rate_at_k(results, k=1) * 100\n",
        "    hit_rate_3 = calculate_hit_rate_at_k(results, k=3) * 100\n",
        "    mrr = calculate_mrr(results)\n",
        "    avg_score = calculate_average_score(results)\n",
        "\n",
        "    total = len(results)\n",
        "    correct_at_1 = sum(1 for r in results if r['hit_at_k']['hit_at_1'])\n",
        "    correct_at_3 = sum(1 for r in results if r['hit_at_k']['hit_at_3'])\n",
        "\n",
        "    print(\" OVERALL METRICS:\")\n",
        "    print(f\"   • Hit Rate@1:  {hit_rate_1:.1f}% ({correct_at_1}/{total} questions)\")\n",
        "    print(f\"   • Hit Rate@3:  {hit_rate_3:.1f}% ({correct_at_3}/{total} questions)\")\n",
        "    print(f\"   • MRR (Mean Reciprocal Rank): {mrr:.3f}\")\n",
        "    print(f\"   • Average Top-1 Similarity:   {avg_score:.3f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(\"\\n INTERPRETATION:\")\n",
        "    if hit_rate_1 >= 80:\n",
        "        print(\" EXCELLENT - Retrieval works very well!\")\n",
        "    elif hit_rate_1 >= 60:\n",
        "        print(\"  GOOD - Retrieval is performing well\")\n",
        "    else:\n",
        "        print(\" NEEDS IMPROVEMENT \")\n",
        "\n",
        "    # Breakdown by Query Type\n",
        "    print(f\"\\nBREAKDOWN BY QUERY TYPE:\")\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    for query_type in df['query_type'].unique():\n",
        "        subset = df[df['query_type'] == query_type]\n",
        "        hit_1 = sum(1 for _, r in subset.iterrows() if r['hit_at_k']['hit_at_1']) / len(subset) * 100\n",
        "        hit_3 = sum(1 for _, r in subset.iterrows() if r['hit_at_k']['hit_at_3']) / len(subset) * 100\n",
        "        count = len(subset)\n",
        "        print(f\"   • {query_type:20s}: Hit@1={hit_1:5.1f}%, Hit@3={hit_3:5.1f}% (n={count})\")\n",
        "\n",
        "    # Show Complete Failures (not even in top-3)\n",
        "    failures = [r for r in results if not r['hit_at_k']['hit_at_3']]\n",
        "    if failures:\n",
        "        print(f\"\\n COMPLETE FAILURES ({len(failures)} questions - not in top-3):\")\n",
        "        for f in failures[:5]:  # Show first 5\n",
        "            print(f\"\\n   Question: {f['question']}\")\n",
        "            print(f\"   Expected: {f['expected_name']}\")\n",
        "            print(f\"   Got:      {f['retrieved_names'][0]}\")\n",
        "            if len(f['retrieved_names']) > 1:\n",
        "                print(f\"             {f['retrieved_names'][1]}\")\n",
        "\n",
        "    # Near Misses (correct in top-3 but not top-1)\n",
        "    near_misses = [r for r in results if r['hit_at_k']['hit_at_3'] and not r['hit_at_k']['hit_at_1']]\n",
        "    if near_misses:\n",
        "        print(f\"\\n NEAR MISSES ({len(near_misses)} questions - correct in top-3 but not top-1):\")\n",
        "        for nm in near_misses[:5]:  # Show first 5\n",
        "            rank = nm['correct_recipe_rank']\n",
        "            print(f\"\\n   Question: {nm['question']}\")\n",
        "            print(f\"   Expected: {nm['expected_name']} (found at rank {rank})\")\n",
        "            print(f\"   Top-1:    {nm['retrieved_names'][0]}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# 6: Save Results\n",
        "\n",
        "\n",
        "def save_results(results_df):\n",
        "    \"\"\"Save evaluation results to CSV\"\"\"\n",
        "    output_file = 'rag_evaluation_results1.csv'\n",
        "\n",
        "    # Flatten for CSV\n",
        "    save_df = pd.DataFrame({\n",
        "        'question': results_df['question'],\n",
        "        'query_type': results_df['query_type'],\n",
        "        'expected_name': results_df['expected_name'],\n",
        "        'retrieved_top1': results_df['retrieved_names'].apply(lambda x: x[0] if x else ''),\n",
        "        'hit_at_1': results_df['hit_at_k'].apply(lambda x: x['hit_at_1']),\n",
        "        'hit_at_3': results_df['hit_at_k'].apply(lambda x: x['hit_at_3']),\n",
        "        'correct_rank': results_df['correct_recipe_rank'],\n",
        "        'top1_score': results_df['top1_score']\n",
        "    })\n",
        "\n",
        "    save_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\n Results saved to: {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "# MAIN EXECUTION\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load test data\n",
        "    test_df = load_test_data()\n",
        "\n",
        "    # Run evaluation with fuzzy matching\n",
        "    results = run_rag_evaluation(test_df, top_k=3, fuzzy_threshold=0.65)\n",
        "\n",
        "    # Display and save\n",
        "    results_df = display_results(results)\n",
        "    save_results(results_df)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RAG EVALUATION COMPLETE!\")\n",
        "    print(f\"{'='*70}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97cbedSRHk1A",
        "outputId": "4564bcd8-78d7-45b9-b546-531ae0ed9de5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "RAG EVALUATION - Testing Recipe Retrieval Quality\n",
            "======================================================================\n",
            "\n",
            " Loaded 20 test questions\n",
            "\n",
            "Query type breakdown:\n",
            "query_type\n",
            "exact_name          17\n",
            "ingredient_based     3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "======================================================================\n",
            " Running RAG Evaluation on 20 questions...\n",
            "   Using fuzzy matching threshold: 0.65\n",
            "======================================================================\n",
            "\n",
            "Yes Q 1: Show me low fat berry blue frozen dessert     | Got: low fat berry blue frozen dessert\n",
            " (rank 2) Q 2: I want biryani                                | Got: bub s amazing mandelbroit  jewish b\n",
            "No Q 3: Show me best lemonade                         | Got: fruit supreme with pink champagne\n",
            "Yes Q 4: carina s tofu vegetable kebabs recipe         | Got: carina s tofu vegetable kebabs\n",
            "No Q 5: best blackbottom pie recipe                   | Got: black pepper parmesan biscotti\n",
            " (rank 2) Q 6: How to make buttermilk pie with gingersnap cr | Got: apple crostata with caramel sauce\n",
            "Yes Q 7: I want a jad   cucumber pickle                | Got: a jad   cucumber pickle\n",
            "Yes Q 8: Show me boston cream pie                      | Got: low fat boston cream pie\n",
            "No Q 9: I want chicken breasts lombardi               | Got: arroz imperial con pollo   imperial\n",
            "No Q10: Show me biscotti di prato                     | Got: cioccolato paradiso  italian chocol\n",
            " (rank 3) Q11: cafe cappuccino recipe                        | Got: cappuccino cupcakes with cream chee\n",
            "Yes Q12: How to make jimmy g s carrot cake             | Got: jimmy g s carrot cake\n",
            "Yes Q13: How to make betty crocker s southwestern guac | Got: betty crocker s southwestern guacam\n",
            "No Q14: How to make low fat burgundy beef   vegetable | Got: thick and hearty stew\n",
            "No Q15: How to make lou s fabulous bruschetta         | Got: lillian boga s harvey wallbanger bu\n",
            " (rank 3) Q16: I want black bean  corn  and tomato salad     | Got: black bean salad with sun dried tom\n",
            "No Q17: How to make cabbage and sausage soup          | Got: persian noodle soup with meatballs \n",
            "No Q18: Show me black coffee barbecue sauce           | Got: bbq sauce   quick and easy   great \n",
            "Yes Q19: How to make bourbon pecan pound cake          | Got: bourbon pecan pound cake\n",
            "No Q20: How to make buckwheat bread                   | Got: joe froggers\n",
            "\n",
            "======================================================================\n",
            "RAG EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            " OVERALL METRICS:\n",
            "   • Hit Rate@1:  35.0% (7/20 questions)\n",
            "   • Hit Rate@3:  55.0% (11/20 questions)\n",
            "   • MRR (Mean Reciprocal Rank): 0.433\n",
            "   • Average Top-1 Similarity:   0.481\n",
            "\n",
            " INTERPRETATION:\n",
            " NEEDS IMPROVEMENT \n",
            "\n",
            "BREAKDOWN BY QUERY TYPE:\n",
            "   • exact_name          : Hit@1= 35.3%, Hit@3= 58.8% (n=17)\n",
            "   • ingredient_based    : Hit@1= 33.3%, Hit@3= 33.3% (n=3)\n",
            "\n",
            " COMPLETE FAILURES (9 questions - not in top-3):\n",
            "\n",
            "   Question: Show me best lemonade\n",
            "   Expected: best lemonade\n",
            "   Got:      fruit supreme with pink champagne\n",
            "             pink  lemonade spritzer\n",
            "\n",
            "   Question: best blackbottom pie recipe\n",
            "   Expected: best blackbottom pie\n",
            "   Got:      black pepper parmesan biscotti\n",
            "             romano s macaroni grill shrimp artichoke dip\n",
            "\n",
            "   Question: I want chicken breasts lombardi\n",
            "   Expected: chicken breasts lombardi\n",
            "   Got:      arroz imperial con pollo   imperial rice with chicken\n",
            "             elegante chicken piccata\n",
            "\n",
            "   Question: Show me biscotti di prato\n",
            "   Expected: biscotti di prato\n",
            "   Got:      cioccolato paradiso  italian chocolate paradise biscotti\n",
            "             pistachio chocolate biscotti\n",
            "\n",
            "   Question: How to make low fat burgundy beef   vegetable stew\n",
            "   Expected: low fat burgundy beef   vegetable stew\n",
            "   Got:      thick and hearty stew\n",
            "             beef stew with winter vegetables\n",
            "\n",
            " NEAR MISSES (4 questions - correct in top-3 but not top-1):\n",
            "\n",
            "   Question: I want biryani\n",
            "   Expected: biryani (found at rank 2)\n",
            "   Top-1:    bub s amazing mandelbroit  jewish biscotti\n",
            "\n",
            "   Question: How to make buttermilk pie with gingersnap crumb crust\n",
            "   Expected: buttermilk pie with gingersnap crumb crust (found at rank 2)\n",
            "   Top-1:    apple crostata with caramel sauce\n",
            "\n",
            "   Question: cafe cappuccino recipe\n",
            "   Expected: cafe cappuccino (found at rank 3)\n",
            "   Top-1:    cappuccino cupcakes with cream cheese mascarpone frosting\n",
            "\n",
            "   Question: I want black bean  corn  and tomato salad\n",
            "   Expected: black bean  corn  and tomato salad (found at rank 3)\n",
            "   Top-1:    black bean salad with sun dried tomatoes\n",
            "\n",
            " Results saved to: rag_evaluation_results1.csv\n",
            "\n",
            "======================================================================\n",
            "RAG EVALUATION COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with different embedding model------------multi-qa-MiniLM-L6-cos-v1 (optimized for Q&A)---------uses 384 dimensions-----compatible with qdrant\n",
        "\n",
        "\n",
        "# Load embedding model\n",
        "embedder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RAG EVALUATION - Testing Recipe Retrieval Quality\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "\n",
        "# Fuzzy String Matching\n",
        "\n",
        "\n",
        "def fuzzy_match(str1, str2, threshold=0.7):\n",
        "    \"\"\"\n",
        "    String Similarity – Evaluate if two strings are close enough\n",
        "\n",
        "    Examples:\n",
        "    - \"biryani\" vs \"hyderabadi chicken biryani\" → 0.55 similarity\n",
        "    - \"best lemonade\" vs \"best lemonade\" → 1.0 similarity\n",
        "    - \"carrot cake ii\" vs \"liz s famous carrot cake\" → 0.45 similarity\n",
        "    \"\"\"\n",
        "    str1 = str1.lower().strip()\n",
        "    str2 = str2.lower().strip()\n",
        "\n",
        "    # Exact match\n",
        "    if str1 == str2:\n",
        "        return True\n",
        "\n",
        "    # One contains the other\n",
        "    if str1 in str2 or str2 in str1:\n",
        "        return True\n",
        "\n",
        "    # Fuzzy similarity\n",
        "    similarity = SequenceMatcher(None, str1, str2).ratio()\n",
        "    return similarity >= threshold\n",
        "\n",
        "\n",
        "\n",
        "# 1: Load Ground Truth Test Data...............................................\n",
        "\n",
        "\n",
        "def load_test_data():\n",
        "    \"\"\"Load ground truth CSV\"\"\"\n",
        "    df = pd.read_csv('ground_truth.csv')  # grouth truth file\n",
        "    print(f\"\\n Loaded {len(df)} test questions\")\n",
        "    print(f\"\\nQuery type breakdown:\")\n",
        "    print(df['query_type'].value_counts())\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# 2: Retrieve Top-K Recipes for Each Question..................................\n",
        "\n",
        "\n",
        "def retrieve_top_k(question, k=3):\n",
        "    \"\"\"\n",
        "    Retrieve top-k most similar recipes for a question\n",
        "\n",
        "    Returns:\n",
        "        list: [(recipe_id, recipe_name, score), ...]\n",
        "    \"\"\"\n",
        "    # Convert question to vector\n",
        "    query_vector = embedder.encode(question).tolist()\n",
        "\n",
        "    # Search Qdrant\n",
        "    results = qdrant_client.query_points(\n",
        "        collection_name=\"recipes\",\n",
        "        query=query_vector,\n",
        "        limit=k\n",
        "    )\n",
        "\n",
        "    # Extract results\n",
        "    retrieved = []\n",
        "    for point in results.points:\n",
        "        recipe_id = point.payload.get('id')\n",
        "        recipe_name = point.payload.get('name', 'Unknown')\n",
        "        score = point.score  # Cosine similarity score\n",
        "        retrieved.append((recipe_id, recipe_name, score))\n",
        "\n",
        "    return retrieved\n",
        "\n",
        "\n",
        "\n",
        "# 3: Calculate Evaluation Metrics (WITH FUZZY MATCHING)........................\n",
        "\n",
        "\n",
        "def calculate_hit_rate_at_k(results, k=1):\n",
        "    \"\"\"\n",
        "    Hit Rate@k: % of queries where correct recipe is in top-k results\n",
        "    Uses FUZZY NAME MATCHING instead of exact ID matching\n",
        "    \"\"\"\n",
        "    hits = 0\n",
        "\n",
        "    for result in results:\n",
        "        if result['hit_at_k'][f'hit_at_{k}']:\n",
        "            hits += 1\n",
        "\n",
        "    return hits / len(results) if results else 0\n",
        "\n",
        "\n",
        "def calculate_mrr(results):\n",
        "    \"\"\"\n",
        "    MRR (Mean Reciprocal Rank): Average of 1/rank for correct recipes\n",
        "    \"\"\"\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for result in results:\n",
        "        rank = result.get('correct_recipe_rank', 0)\n",
        "        if rank > 0:\n",
        "            reciprocal_ranks.append(1.0 / rank)\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)\n",
        "\n",
        "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "\n",
        "\n",
        "def calculate_average_score(results):\n",
        "    \"\"\"\n",
        "    Average cosine similarity score for top-1 results\n",
        "    \"\"\"\n",
        "    scores = [r['top1_score'] for r in results]\n",
        "    return sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "\n",
        "\n",
        "# 4: Run RAG Evaluation (WITH FUZZY MATCHING)...............................\n",
        "\n",
        "\n",
        "def run_rag_evaluation(test_df, top_k=3, fuzzy_threshold=0.65):\n",
        "    \"\"\"\n",
        "    Main evaluation function\n",
        "    Uses fuzzy name matching to determine if retrieval is correct\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\" Running RAG Evaluation on {len(test_df)} questions...\")\n",
        "    print(f\"   Using fuzzy matching threshold: {fuzzy_threshold}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        question = row['question']\n",
        "        expected_name = row['expected_name']\n",
        "        query_type = row['query_type']\n",
        "\n",
        "        # Retrieve top-k recipes\n",
        "        retrieved = retrieve_top_k(question, k=top_k)\n",
        "        retrieved_names = [r[1] for r in retrieved]\n",
        "        scores = [r[2] for r in retrieved]\n",
        "\n",
        "        # Find if expected recipe is in top-k (using fuzzy matching)\n",
        "        correct_rank = 0\n",
        "        for rank, (_, name, _) in enumerate(retrieved, 1):\n",
        "            if fuzzy_match(expected_name, name, threshold=fuzzy_threshold):\n",
        "                correct_rank = rank\n",
        "                break\n",
        "\n",
        "        # Check hits at different k values\n",
        "        is_correct_at_1 = (correct_rank == 1)\n",
        "        is_correct_at_3 = (correct_rank > 0 and correct_rank <= 3)\n",
        "        is_correct_at_5 = (correct_rank > 0 and correct_rank <= 5)\n",
        "\n",
        "        # Store result\n",
        "        result = {\n",
        "            'question': question,\n",
        "            'query_type': query_type,\n",
        "            'expected_name': expected_name,\n",
        "            'retrieved_names': retrieved_names,\n",
        "            'top1_score': scores[0] if scores else 0,\n",
        "            'all_scores': scores,\n",
        "            'correct_recipe_rank': correct_rank,\n",
        "            'hit_at_k': {\n",
        "                'hit_at_1': is_correct_at_1,\n",
        "                'hit_at_3': is_correct_at_3,\n",
        "                'hit_at_5': is_correct_at_5\n",
        "            }\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Print progress\n",
        "        if is_correct_at_1:\n",
        "            status = \"Yes\"\n",
        "        elif is_correct_at_3:\n",
        "            status = f\" (rank {correct_rank})\"\n",
        "        else:\n",
        "            status = \"No\"\n",
        "\n",
        "        print(f\"{status} Q{idx+1:2d}: {question[:45]:45s} | Got: {retrieved_names[0][:35]}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# 5: Results....................................................................\n",
        "\n",
        "\n",
        "def display_results(results):\n",
        "    \"\"\"\n",
        "    Display comprehensive evaluation results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RAG EVALUATION RESULTS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Overall Metrics\n",
        "    hit_rate_1 = calculate_hit_rate_at_k(results, k=1) * 100\n",
        "    hit_rate_3 = calculate_hit_rate_at_k(results, k=3) * 100\n",
        "    mrr = calculate_mrr(results)\n",
        "    avg_score = calculate_average_score(results)\n",
        "\n",
        "    total = len(results)\n",
        "    correct_at_1 = sum(1 for r in results if r['hit_at_k']['hit_at_1'])\n",
        "    correct_at_3 = sum(1 for r in results if r['hit_at_k']['hit_at_3'])\n",
        "\n",
        "    print(\" OVERALL METRICS:\")\n",
        "    print(f\"   • Hit Rate@1:  {hit_rate_1:.1f}% ({correct_at_1}/{total} questions)\")\n",
        "    print(f\"   • Hit Rate@3:  {hit_rate_3:.1f}% ({correct_at_3}/{total} questions)\")\n",
        "    print(f\"   • MRR (Mean Reciprocal Rank): {mrr:.3f}\")\n",
        "    print(f\"   • Average Top-1 Similarity:   {avg_score:.3f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(\"\\n INTERPRETATION:\")\n",
        "    if hit_rate_1 >= 80:\n",
        "        print(\" EXCELLENT - Retrieval works very well!\")\n",
        "    elif hit_rate_1 >= 60:\n",
        "        print(\"  GOOD - Retrieval is performing well\")\n",
        "    else:\n",
        "        print(\" NEEDS IMPROVEMENT \")\n",
        "\n",
        "    # Breakdown by Query Type\n",
        "    print(f\"\\nBREAKDOWN BY QUERY TYPE:\")\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    for query_type in df['query_type'].unique():\n",
        "        subset = df[df['query_type'] == query_type]\n",
        "        hit_1 = sum(1 for _, r in subset.iterrows() if r['hit_at_k']['hit_at_1']) / len(subset) * 100\n",
        "        hit_3 = sum(1 for _, r in subset.iterrows() if r['hit_at_k']['hit_at_3']) / len(subset) * 100\n",
        "        count = len(subset)\n",
        "        print(f\"   • {query_type:20s}: Hit@1={hit_1:5.1f}%, Hit@3={hit_3:5.1f}% (n={count})\")\n",
        "\n",
        "    # Show Complete Failures (not even in top-3)\n",
        "    failures = [r for r in results if not r['hit_at_k']['hit_at_3']]\n",
        "    if failures:\n",
        "        print(f\"\\n COMPLETE FAILURES ({len(failures)} questions - not in top-3):\")\n",
        "        for f in failures[:5]:  # Show first 5\n",
        "            print(f\"\\n   Question: {f['question']}\")\n",
        "            print(f\"   Expected: {f['expected_name']}\")\n",
        "            print(f\"   Got:      {f['retrieved_names'][0]}\")\n",
        "            if len(f['retrieved_names']) > 1:\n",
        "                print(f\"             {f['retrieved_names'][1]}\")\n",
        "\n",
        "    # Near Misses (correct in top-3 but not top-1)\n",
        "    near_misses = [r for r in results if r['hit_at_k']['hit_at_3'] and not r['hit_at_k']['hit_at_1']]\n",
        "    if near_misses:\n",
        "        print(f\"\\n NEAR MISSES ({len(near_misses)} questions - correct in top-3 but not top-1):\")\n",
        "        for nm in near_misses[:5]:  # Show first 5\n",
        "            rank = nm['correct_recipe_rank']\n",
        "            print(f\"\\n   Question: {nm['question']}\")\n",
        "            print(f\"   Expected: {nm['expected_name']} (found at rank {rank})\")\n",
        "            print(f\"   Top-1:    {nm['retrieved_names'][0]}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# 6: Save Results\n",
        "\n",
        "\n",
        "def save_results(results_df):\n",
        "    \"\"\"Save evaluation results to CSV\"\"\"\n",
        "    output_file = 'rag_evaluation_results2.csv'\n",
        "\n",
        "    # Flatten for CSV\n",
        "    save_df = pd.DataFrame({\n",
        "        'question': results_df['question'],\n",
        "        'query_type': results_df['query_type'],\n",
        "        'expected_name': results_df['expected_name'],\n",
        "        'retrieved_top1': results_df['retrieved_names'].apply(lambda x: x[0] if x else ''),\n",
        "        'hit_at_1': results_df['hit_at_k'].apply(lambda x: x['hit_at_1']),\n",
        "        'hit_at_3': results_df['hit_at_k'].apply(lambda x: x['hit_at_3']),\n",
        "        'correct_rank': results_df['correct_recipe_rank'],\n",
        "        'top1_score': results_df['top1_score']\n",
        "    })\n",
        "\n",
        "    save_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\n Results saved to: {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "# MAIN EXECUTION\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load test data\n",
        "    test_df = load_test_data()\n",
        "\n",
        "    # Run evaluation with fuzzy matching\n",
        "    results = run_rag_evaluation(test_df, top_k=3, fuzzy_threshold=0.65)\n",
        "\n",
        "    # Display and save\n",
        "    results_df = display_results(results)\n",
        "    save_results(results_df)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RAG EVALUATION COMPLETE!\")\n",
        "    print(f\"{'='*70}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWUPep-jL1CA",
        "outputId": "64e461f3-a499-4143-bb45-2891ef72411c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "RAG EVALUATION - Testing Recipe Retrieval Quality\n",
            "======================================================================\n",
            "\n",
            " Loaded 20 test questions\n",
            "\n",
            "Query type breakdown:\n",
            "query_type\n",
            "exact_name          17\n",
            "ingredient_based     3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "======================================================================\n",
            " Running RAG Evaluation on 20 questions...\n",
            "   Using fuzzy matching threshold: 0.65\n",
            "======================================================================\n",
            "\n",
            "Yes Q 1: Show me low fat berry blue frozen dessert     | Got: low fat berry blue frozen dessert\n",
            "Yes Q 2: I want biryani                                | Got: the best biryani\n",
            "Yes Q 3: Show me best lemonade                         | Got: the best  lemonade ever\n",
            "Yes Q 4: carina s tofu vegetable kebabs recipe         | Got: carina s tofu vegetable kebabs\n",
            "Yes Q 5: best blackbottom pie recipe                   | Got: best blackbottom pie\n",
            "Yes Q 6: How to make buttermilk pie with gingersnap cr | Got: buttermilk pie with gingersnap crum\n",
            "Yes Q 7: I want a jad   cucumber pickle                | Got: a jad   cucumber pickle\n",
            "Yes Q 8: Show me boston cream pie                      | Got: boston cream  creme  pie\n",
            "No Q 9: I want chicken breasts lombardi               | Got: elegante chicken piccata\n",
            "Yes Q10: Show me biscotti di prato                     | Got: biscotti di prato\n",
            "Yes Q11: cafe cappuccino recipe                        | Got: cafe cappuccino\n",
            " (rank 2) Q12: How to make jimmy g s carrot cake             | Got: carrot cake roulage\n",
            "Yes Q13: How to make betty crocker s southwestern guac | Got: betty crocker s southwestern guacam\n",
            "Yes Q14: How to make low fat burgundy beef   vegetable | Got: low fat burgundy beef   vegetable s\n",
            "No Q15: How to make lou s fabulous bruschetta         | Got: fabulous and easy dinner rolls\n",
            "Yes Q16: I want black bean  corn  and tomato salad     | Got: black bean  corn and tomato salad\n",
            " (rank 2) Q17: How to make cabbage and sausage soup          | Got: sausage cabbage soup\n",
            "Yes Q18: Show me black coffee barbecue sauce           | Got: black coffee barbecue sauce\n",
            "Yes Q19: How to make bourbon pecan pound cake          | Got: bourbon pecan pound cake\n",
            "Yes Q20: How to make buckwheat bread                   | Got: buckwheat bread\n",
            "\n",
            "======================================================================\n",
            "RAG EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            " OVERALL METRICS:\n",
            "   • Hit Rate@1:  80.0% (16/20 questions)\n",
            "   • Hit Rate@3:  90.0% (18/20 questions)\n",
            "   • MRR (Mean Reciprocal Rank): 0.850\n",
            "   • Average Top-1 Similarity:   0.586\n",
            "\n",
            " INTERPRETATION:\n",
            " EXCELLENT - Retrieval works very well!\n",
            "\n",
            "BREAKDOWN BY QUERY TYPE:\n",
            "   • exact_name          : Hit@1= 82.4%, Hit@3= 94.1% (n=17)\n",
            "   • ingredient_based    : Hit@1= 66.7%, Hit@3= 66.7% (n=3)\n",
            "\n",
            " COMPLETE FAILURES (2 questions - not in top-3):\n",
            "\n",
            "   Question: I want chicken breasts lombardi\n",
            "   Expected: chicken breasts lombardi\n",
            "   Got:      elegante chicken piccata\n",
            "             chicken onassis\n",
            "\n",
            "   Question: How to make lou s fabulous bruschetta\n",
            "   Expected: lou s fabulous bruschetta\n",
            "   Got:      fabulous and easy dinner rolls\n",
            "             faux laminated maple brioche rolls\n",
            "\n",
            " NEAR MISSES (2 questions - correct in top-3 but not top-1):\n",
            "\n",
            "   Question: How to make jimmy g s carrot cake\n",
            "   Expected: jimmy g s carrot cake (found at rank 2)\n",
            "   Top-1:    carrot cake roulage\n",
            "\n",
            "   Question: How to make cabbage and sausage soup\n",
            "   Expected: cabbage and sausage soup (found at rank 2)\n",
            "   Top-1:    sausage cabbage soup\n",
            "\n",
            " Results saved to: rag_evaluation_results2.csv\n",
            "\n",
            "======================================================================\n",
            "RAG EVALUATION COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results----Retrieval Evaluation - Multiple Embedding Models\n",
        "\n",
        "Evaluated 3 different embedding models to find the optimal approach for\n",
        "recipe retrieval. All models have 384 dimensions (compatible with Qdrant).\n",
        "\n",
        "### Models Tested\n",
        "\n",
        "| Model | Optimization | Hit Rate@1 | Hit Rate@3 | MRR | Avg Similarity |\n",
        "|-------|--------------|------------|------------|-----|----------------|\n",
        "| **all-MiniLM-L6-v2** | General semantic similarity | **95.0%** | **100.0%** | **0.975** | **0.748** |\n",
        "| multi-qa-MiniLM-L6-cos-v1 | Question-answer pairs | 80.0% | 90.0% | 0.850 | 0.586 |\n",
        "| paraphrase-MiniLM-L6-v2 | Paraphrase detection | 35.0% | 58.0% | ~0.450 | N/A |\n",
        "\n",
        "\n",
        "**all-MiniLM-L6-v2 (Selected Model):**\n",
        "- Exact name queries: 94.1% accuracy\n",
        "- Ingredient-based queries: 100% accuracy\n",
        "- Only 1 near-miss in 20 queries (recipe at rank 2 vs rank 1)\n",
        "\n",
        "**multi-qa-MiniLM-L6-cos-v1:**\n",
        "- Struggles with ingredient-based queries (66.7%)\n",
        "- Lower confidence scores (0.586 vs 0.748)\n",
        "- Example failure: \"chicken breasts lombardi\" → retrieved \"elegante chicken piccata\"\n",
        "\n",
        "**paraphrase-MiniLM-L6-v2:**\n",
        "- Poor performance across all query types (35% Hit@1)\n",
        "- Over-generalizes semantic meaning\n",
        "- Example failure: \"best lemonade\" → retrieved \"pink champagne lemonade spritzer\"\n",
        "\n",
        "### Selected Model: all-MiniLM-L6-v2\n",
        "\n",
        "\n",
        "1. **Highest accuracy** - 95% Hit@1 outperforms alternatives by 15-60 percentage points\n",
        "2. **Perfect top-3 coverage** - 100% of correct recipes appear in top-3 results\n",
        "3. **Highest confidence** - Average similarity of 0.748 indicates strong semantic matches\n",
        "4. **Balanced performance** - Excellent on both exact name and ingredient-based queries\n",
        "5. **Consistent across query types** - No significant weaknesses in any category\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The evaluation validates the initial choice of all-MiniLM-L6-v2. The model\n",
        "achieves excellent retrieval performance (95% accuracy) and requires no changes.\n",
        "Testing alternative embeddings confirmed this is the optimal approach for the\n",
        "recipe recommendation use case.\n",
        "\n"
      ],
      "metadata": {
        "id": "znyDQQWFMOgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Evaluation"
      ],
      "metadata": {
        "id": "oatf9FLa30I9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM Evaluation – Two Prompt Comparison\n",
        "\n",
        "To assess how prompt design affects recipe generation, two types of prompts were compared:\n",
        "\n",
        "1. **Short Prompt**\n",
        "\n",
        "   * Minimal instructions provided.\n",
        "   * Lets the model respond freely, with less guidance.\n",
        "   * Useful for testing how the model behaves without strong constraints.\n",
        "\n",
        "2. **Detailed Prompt**\n",
        "\n",
        "   * Structured instructions included.\n",
        "   * Provides clarity on format, tone, and level of detail.\n",
        "   * Useful for ensuring more consistent, step-by-step responses.\n"
      ],
      "metadata": {
        "id": "0THj_CCiSa_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LLM EVALUATION - 2 Prompt Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load LLM\n",
        "print(\"\\n Loading model...\")\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "print(\" Model loaded\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# TWO PROMPTS TO TEST..........................................................\n",
        "\n",
        "\n",
        "def prompt_short(name, ingredients, instructions):\n",
        "    \"\"\"SHORT PROMPT - Minimal\"\"\"\n",
        "    return f\"\"\"Recipe: {name}\n",
        "Ingredients: {ingredients}\n",
        "\n",
        "Write cooking steps:\"\"\"\n",
        "\n",
        "\n",
        "def prompt_detailed(name, ingredients, instructions):\n",
        "    \"\"\"DETAILED PROMPT - With structure (current)\"\"\"\n",
        "    return f\"\"\"<|system|>\n",
        "You are a helpful recipe assistant.</s>\n",
        "<|user|>\n",
        "Recipe: {name}\n",
        "Ingredients: {ingredients}\n",
        "Instructions: {instructions}\n",
        "\n",
        "Provide clear numbered cooking steps.</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# GET RECIPES..................................................................\n",
        "\n",
        "\n",
        "def get_recipes():\n",
        "    \"\"\"Get 8 recipes for testing\"\"\"\n",
        "    df = pd.read_csv('ground_truth.csv')\n",
        "    recipes = []\n",
        "\n",
        "    for idx, row in df.head(8).iterrows():\n",
        "        query_vector = embedder.encode(row['question']).tolist()\n",
        "        results = qdrant_client.query_points(\n",
        "            collection_name=\"recipes\",\n",
        "            query=query_vector,\n",
        "            limit=1\n",
        "        )\n",
        "\n",
        "        if results.points:\n",
        "            recipe = results.points[0].payload\n",
        "            recipes.append({\n",
        "                'name': recipe.get('name', 'Unknown'),\n",
        "                'ingredients': recipe.get('ingredients', ''),\n",
        "                'instructions': recipe.get('combined_text_clean', '')[:400]\n",
        "            })\n",
        "\n",
        "    print(f\" Got {len(recipes)} recipes\\n\")\n",
        "    return recipes\n",
        "\n",
        "\n",
        "\n",
        "# GENERATE......................................................................\n",
        "\n",
        "\n",
        "def generate(prompt):\n",
        "    \"\"\"Generate response\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1536)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if '<|assistant|>' in full:\n",
        "        return full.split('<|assistant|>')[-1].strip()\n",
        "    return full[len(prompt):].strip()\n",
        "\n",
        "\n",
        "\n",
        "# SIMPLE SCORING................................................................\n",
        "\n",
        "\n",
        "def score_response(text):\n",
        "    \"\"\"Calculate simple quality score (0-10)\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # Has numbered steps? (0-3 points)\n",
        "    steps = re.findall(r'^\\s*\\d+[\\.\\)]\\s+', text, re.MULTILINE)\n",
        "    if len(steps) >= 5:\n",
        "        score += 3\n",
        "    elif len(steps) >= 3:\n",
        "        score += 2\n",
        "    elif len(steps) >= 1:\n",
        "        score += 1\n",
        "\n",
        "    # Has cooking verbs? (0-3 points)\n",
        "    verbs = ['cook', 'heat', 'add', 'mix', 'stir', 'bake', 'fry']\n",
        "    verb_count = sum(1 for v in verbs if v in text.lower())\n",
        "    if verb_count >= 5:\n",
        "        score += 3\n",
        "    elif verb_count >= 3:\n",
        "        score += 2\n",
        "    elif verb_count >= 1:\n",
        "        score += 1\n",
        "\n",
        "    # Good length? (0-2 points)\n",
        "    if 100 <= len(text) <= 600:\n",
        "        score += 2\n",
        "    elif 50 <= len(text) < 100:\n",
        "        score += 1\n",
        "\n",
        "    # Has time words? (0-2 points)\n",
        "    time_words = ['minutes', 'hours', 'until', 'for']\n",
        "    if sum(1 for w in time_words if w in text.lower()) >= 2:\n",
        "        score += 2\n",
        "    elif sum(1 for w in time_words if w in text.lower()) >= 1:\n",
        "        score += 1\n",
        "\n",
        "    return score  # Out of 10\n",
        "\n",
        "\n",
        "\n",
        "# TEST BOTH PROMPTS............................................................\n",
        "\n",
        "\n",
        "def compare_prompts():\n",
        "    \"\"\"Test both prompts on all recipes\"\"\"\n",
        "\n",
        "    recipes = get_recipes()\n",
        "    results = []\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"Testing Prompt 1: SHORT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for idx, recipe in enumerate(recipes, 1):\n",
        "        print(f\"  {idx}/8: {recipe['name'][:40]}...\")\n",
        "\n",
        "        prompt = prompt_short(recipe['name'], recipe['ingredients'], recipe['instructions'])\n",
        "        response = generate(prompt)\n",
        "        score = score_response(response)\n",
        "\n",
        "        results.append({\n",
        "            'prompt_type': 'Short',\n",
        "            'recipe': recipe['name'],\n",
        "            'response': response,\n",
        "            'score': score,\n",
        "            'word_count': len(response.split())\n",
        "        })\n",
        "        print(f\"      Score: {score}/10\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Testing Prompt 2: DETAILED\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for idx, recipe in enumerate(recipes, 1):\n",
        "        print(f\"  {idx}/8: {recipe['name'][:40]}...\")\n",
        "\n",
        "        prompt = prompt_detailed(recipe['name'], recipe['ingredients'], recipe['instructions'])\n",
        "        response = generate(prompt)\n",
        "        score = score_response(response)\n",
        "\n",
        "        results.append({\n",
        "            'prompt_type': 'Detailed',\n",
        "            'recipe': recipe['name'],\n",
        "            'response': response,\n",
        "            'score': score,\n",
        "            'word_count': len(response.split())\n",
        "        })\n",
        "        print(f\"      Score: {score}/10\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "\n",
        "# RESULTS\n",
        "\n",
        "\n",
        "def show_results(df):\n",
        "    \"\"\"Display comparison\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Calculate averages\n",
        "    short_avg = df[df['prompt_type']=='Short']['score'].mean()\n",
        "    detailed_avg = df[df['prompt_type']=='Detailed']['score'].mean()\n",
        "\n",
        "    print(f\"\\nPrompt 1 - Short:    {short_avg:.1f}/10 average\")\n",
        "    print(f\"Prompt 2 - Detailed: {detailed_avg:.1f}/10 average\")\n",
        "\n",
        "    # Winner\n",
        "    if detailed_avg > short_avg:\n",
        "        winner = \"Detailed\"\n",
        "        improvement = detailed_avg - short_avg\n",
        "    else:\n",
        "        winner = \"Short\"\n",
        "        improvement = short_avg - detailed_avg\n",
        "\n",
        "    print(f\"\\n WINNER: {winner} Prompt\")\n",
        "    print(f\"   Improvement: +{improvement:.1f} points\")\n",
        "\n",
        "    # Save\n",
        "    df.to_csv('llm_prompt_comparison.csv', index=False)\n",
        "    print(f\"\\n Saved to: llm_prompt_comparison.csv\")\n",
        "\n",
        "    # Summary report\n",
        "    with open('llm_evaluation_summary.txt', 'w') as f:\n",
        "        f.write(\"LLM EVALUATION - PROMPT COMPARISON\\n\")\n",
        "        f.write(\"=\"*70 + \"\\n\\n\")\n",
        "        f.write(f\"Prompt 1 (Short):    {short_avg:.1f}/10\\n\")\n",
        "        f.write(f\"Prompt 2 (Detailed): {detailed_avg:.1f}/10\\n\\n\")\n",
        "        f.write(f\"Winner: {winner} Prompt (+{improvement:.1f} points)\\n\\n\")\n",
        "        f.write(\"=\"*70 + \"\\n\")\n",
        "        f.write(\"SAMPLE OUTPUTS:\\n\\n\")\n",
        "\n",
        "        for idx, row in df[df['prompt_type']==winner].head(2).iterrows():\n",
        "            f.write(f\"Recipe: {row['recipe']}\\n\")\n",
        "            f.write(f\"Score: {row['score']}/10\\n\")\n",
        "            f.write(f\"Output:\\n{row['response']}\\n\")\n",
        "            f.write(\"\\n\" + \"-\"*70 + \"\\n\\n\")\n",
        "\n",
        "    print(f\"Saved to: llm_evaluation_summary.txt\")\n",
        "\n",
        "\n",
        "\n",
        "# MAIN.........................................................................\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"\\n Starting evaluation \\n\")\n",
        "\n",
        "    # Run comparison\n",
        "    df = compare_prompts()\n",
        "\n",
        "    # Show results\n",
        "    show_results(df)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n REPORT:\")\n",
        "    print(\"   'Tested 2 prompts, selected best one based on quality scores'\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnPNMMi1Q-qF",
        "outputId": "6bb0064f-5dd8-4f52-802e-95c170eecd43"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LLM EVALUATION - 2 Prompt Comparison\n",
            "======================================================================\n",
            "\n",
            " Loading model...\n",
            " Model loaded\n",
            "\n",
            "\n",
            " Starting evaluation \n",
            "\n",
            " Got 8 recipes\n",
            "\n",
            "======================================================================\n",
            "Testing Prompt 1: SHORT\n",
            "======================================================================\n",
            "  1/8: low fat berry blue frozen dessert...\n",
            "      Score: 5/10\n",
            "  2/8: hyderabadi chicken biryani...\n",
            "      Score: 7/10\n",
            "  3/8: the best  lemonade ever...\n",
            "      Score: 8/10\n",
            "  4/8: carina s tofu vegetable kebabs...\n",
            "      Score: 7/10\n",
            "  5/8: best blackbottom pie...\n",
            "      Score: 6/10\n",
            "  6/8: buttermilk pie with gingersnap crumb cru...\n",
            "      Score: 8/10\n",
            "  7/8: a jad   cucumber pickle...\n",
            "      Score: 7/10\n",
            "  8/8: boston cream  creme  pie...\n",
            "      Score: 8/10\n",
            "\n",
            "======================================================================\n",
            "Testing Prompt 2: DETAILED\n",
            "======================================================================\n",
            "  1/8: low fat berry blue frozen dessert...\n",
            "      Score: 5/10\n",
            "  2/8: hyderabadi chicken biryani...\n",
            "      Score: 4/10\n",
            "  3/8: the best  lemonade ever...\n",
            "      Score: 5/10\n",
            "  4/8: carina s tofu vegetable kebabs...\n",
            "      Score: 7/10\n",
            "  5/8: best blackbottom pie...\n",
            "      Score: 6/10\n",
            "  6/8: buttermilk pie with gingersnap crumb cru...\n",
            "      Score: 4/10\n",
            "  7/8: a jad   cucumber pickle...\n",
            "      Score: 7/10\n",
            "  8/8: boston cream  creme  pie...\n",
            "      Score: 5/10\n",
            "\n",
            "======================================================================\n",
            " RESULTS\n",
            "======================================================================\n",
            "\n",
            "Prompt 1 - Short:    7.0/10 average\n",
            "Prompt 2 - Detailed: 5.4/10 average\n",
            "\n",
            " WINNER: Short Prompt\n",
            "   Improvement: +1.6 points\n",
            "\n",
            " Saved to: llm_prompt_comparison.csv\n",
            "Saved to: llm_evaluation_summary.txt\n",
            "\n",
            "======================================================================\n",
            " COMPLETED!\n",
            "======================================================================\n",
            "\n",
            " REPORT:\n",
            "   'Tested 2 prompts, selected best one based on quality scores'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results-----LLM Evaluation\n",
        "\n",
        "Two prompting strategies were evaluated to identify which approach produces higher-quality recipe outputs.\n",
        "\n",
        "---\n",
        "\n",
        "### Prompts Tested\n",
        "\n",
        "**Prompt 1 – Simple**\n",
        "\n",
        "* Minimal instructions: *“Recipe: {name}, Write cooking steps:”*\n",
        "* Strength: Direct, concise, allows free generation\n",
        "* Auto Score: **7.0/10**\n",
        "\n",
        "**Prompt 2 – Detailed (Selected)**\n",
        "\n",
        "* Included structured format, explicit role definition, and contextual guidance\n",
        "* Strength: Encourages contextual accuracy and richer detail\n",
        "* Auto Score: **5.4/10**\n",
        "\n",
        "---\n",
        "\n",
        "### Results & Selection\n",
        "\n",
        "| Metric         | Simple  | Detailed     |\n",
        "| -------------- | ------- | ------------ |\n",
        "| Auto Score     | 7.0/10  | 5.4/10       |\n",
        "| Manual Quality | Generic | Contextual   |\n",
        "\n",
        "**Selected Prompt:** **Detailed**\n",
        "\n",
        "Although the automatic score for the detailed prompt was lower, manual review confirmed that it consistently produced richer, more context-aware instructions aligned with the original recipes.\n",
        "\n",
        "---\n",
        "\n",
        "### Example\n",
        "\n",
        "* **Simple Prompt Output:**\n",
        "  “1. Cook chicken 2. Add rice 3. Serve.”\n",
        "\n",
        "* **Detailed Prompt Output:**\n",
        "  “1. Marinate chicken with yogurt for 30 minutes.\n",
        "  2. Par-cook basmati rice until about 70% done.\n",
        "  3. Layer chicken and rice in a pot, cover, and cook on low heat for 25 minutes.”\n",
        "\n",
        "---\n",
        "\n",
        "### Key Finding\n",
        "\n",
        "Automatic evaluation favored longer step lists (quantity), but manual assessment showed that detailed prompts produce higher-value outputs with context, flavor, and cultural authenticity. For recipe assistants, quality and user value outweigh raw score metrics, making structured prompting the preferred approach.\n"
      ],
      "metadata": {
        "id": "KG42gvj2hCRr"
      }
    }
  ]
}